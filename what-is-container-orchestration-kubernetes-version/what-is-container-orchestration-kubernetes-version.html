<h1>What is Container Orchestration - Kubernetes Version?</h1>
<p>In a previous post, <a href="https://capstonec.com/what-is-container-orchestration/">What is Container Orchestration?</a>, I explained container orchestration using some examples based on Docker Swarm. While Docker Swarm is undeniably easier to both use and explain, <a href="https://kubernetes.io/">Kubernetes</a> is by far the most prevalent container orchestrator today. So, I'm going to go through the same examples from that previous post but, this time, use Kubernetes. One of the great things about <a href="https://www.docker.com/products/docker-enterprise">Docker Enterprise</a> is it supports both Swarm and Kubernetes so I didn't have to change my infrastructure at all.</p>
<h2>Visualizing Orchestration</h2>
<p>I used the <a href="https://github.com/dockersamples/docker-swarm-visualizer">Docker Swarm Visualizer</a> in the videos of the last post to help you visualize what was happening. For visualizing Kubernetes, I tried Brendan Burns' <a href="https://github.com/brendandburns/gcp-live-k8s-visualizer">gcp-live-k8s-visualizer</a> and <a href="https://github.com/weaveworks/scope">Weaveworks Scope</a>. I found the former doesn't tie in the nodes enough and the latter has too much for simple demos. However, Scope has a lot of capabilities I'd like to explore further so I used it in the videos below. Installing Scope into our Kubernetes cluster is easy. See https://www.weave.works/docs/scope/latest/installing/#k8s.</p>
<h2>Taints and Tolerations</h2>
<p>With Swarm I used node labels to designate two of the worker nodes in my cluster as being in my private cloud and two in my public cloud. Then, when I created the Swarm service, I used constraints to only run the service (initially) in my private cloud. In Kubernetes, the (rough) equivalent to labels are taints and the (rough) equivalent to constraints are tolerations. There are a lot more uses for taints and tolerations. If you want to learn more about them, see <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Taints and Tolerations</a> in the Kubernetes documentation.</p>
<p>In our case, we are going to taint two of our worker nodes with the key-value pair of <code>cloud=private</code> and the effect of <code>NoSchedule</code>. For the other two, we use the key-value pair of <code>cloud=public</code> and the same effect. In essence, this tells the scheduler to not schedule anything on that node unless it has a toleration for the specified key-value pair.</p>
<pre><code class="language-bash">$ kubectl taint nodes ip-172-30-14-227.us-east-2.compute.internal cloud=private:NoSchedule
$ kubectl taint nodes ip-172-30-14-227.us-east-2.compute.internal cloud=private:NoSchedule
$ kubectl taint nodes ip-172-30-23-45.us-east-2.compute.internal cloud=public:NoSchedule
$ kubectl taint nodes ip-172-30-23-45.us-east-2.compute.internal cloud=public:NoSchedule
</code></pre>
<h2>Demonstrate Deploying, Scaling, and Upgrading</h2>
<p>Once again, we start by creating a service using the official NGINX 1.14 image. The service will have replicas running in my private cloud. We will accomplish this by applying the following resource configuration files, kens-deployment.yaml and kens-service.yaml. The first creates the replica set responsible for the pods with the NGINX containers and the second creates a load balancer service so we can access it.</p>
<pre><code class="language-yaml">kind: Deployment
apiVersion: apps/v1
metadata:
  name: kens-deployment
  labels:
    visualize: &quot;true&quot;
    run: nginx
  namespace: development
spec:
  selector:
    matchLabels:
      app: kens-app
  replicas: 2
  template:
    metadata:
      labels:
        app: kens-app
        visualize: &quot;true&quot;
        run: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14
        ports:
        - containerPort: 80
      tolerations:
      - key: &quot;cloud&quot;
        operator: &quot;Equal&quot;
        value: &quot;private&quot;
        effect: &quot;NoSchedule&quot;
</code></pre>
<pre><code class="language-bash">$ kubectl apply -f kens-deployment.yaml
</code></pre>
<pre><code class="language-yaml">kind: Service
apiVersion: v1
metadata:
  name: kens-service
  labels:
    visualize: &quot;true&quot;
    run: nginx
  namespace: development
spec:
  type: LoadBalancer
  selector:
    app: kens-app
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
</code></pre>
<pre><code class="language-bash">$ kubectl apply -f kens-service.yaml
</code></pre>
<p>Notice the pods created are only running on the worker nodes with the <code>cloud=private</code> taint.</p>
<p>Next, we'll scale the replica set from 2 to 4 replicas. We'll do this by updating the number of replicas specified in kens-deployment.yaml and applying it.</p>
<pre><code class="language-yaml">  replicas: 4
</code></pre>
<pre><code class="language-bash">$ kubectl apply -f kens-deployment.yaml
</code></pre>
<p>We now have 4 pods running and they're all on the private cloud worker nodes.</p>
<p>In my previous post, to allow the pods to run on both the private and public cloud worker nodes, we removed the private cloud constraint. However, with taints, we need to add a toleration for the public cloud to the pod specification in the deployment specification.</p>
<pre><code class="language-yaml">      tolerations:
      - key: &quot;cloud&quot;
        operator: &quot;Equal&quot;
        value: &quot;private&quot;
        effect: &quot;NoSchedule&quot;
      - key: &quot;cloud&quot;
        operator: &quot;Equal&quot;
        value: &quot;public&quot;
        effect: &quot;NoSchedule&quot;
</code></pre>
<pre><code class="language-bash">$ kubectl apply -f kens-deployment.yaml
</code></pre>
<p>This leads to a difference between Swarm and Kubernetes orchestration. When we took the equivalent action in Swarm, the scheduler, essentially, did nothing with the running containers. Since we removed a constraint from the service and didn't change the container specification, the scheduler didn't have to do anything as the current state of the running containers matched the desired state. In the Kubernetes case, we're making a change to the pod specification so the current state of the running pods doesn't match the desired state so the scheduler creates a new replica set and removes the previous one. As a result, we end up with pods running on both the private and public cloud worker nodes. In the Swarm example, we had to scale up the number of replicas to make that happen.</p>
<p>Finally, let's upgrade NGINX from 1.14 to 1.15. Again, we update the image tag in the pod specification of the deployment and apply it.</p>
<pre><code class="language-yaml">    spec:
      containers:
      - name: nginx
        image: nginx:1.15
</code></pre>
<pre><code class="language-bash">$ kubectl apply -f kens-deployment.yaml
</code></pre>
<h2>Demonstrate Failures</h2>
<p>We'll start by demonstrating an all too typical upgrade failure scenario. As with Swarm, Kubernetes has quite a few options for detecting upgrade failures and automatically rolling back to the previous version. In this case, we're going to assume the upgrade succeeded but we found a problem post-upgrade. There are still several options available to us. You could use the <code>kubectl rollout</code> feature for deployments. (See <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a> in the Kubernetes documentation.) However, we're a big believer in our current state matching our desired state along with matching what we have under source control. So, we'll update the deployment specification with an old image tag and apply it. (Or, maybe, we'll revert the change in our source and our CI/CD pipeline will apply it for us.) In any case, we'll see a rolling update to the pods.</p>
<p>Now, we'll demonstrate the failure of a container by deleting a pod. You will see the scheduler notices almost immediately that the current state, i.e. 3 pods, doesn't match the desired state, i.e. 4 pods, so it starts another one.</p>
<p>To simulate a server failure, I'm going to shutdown one of the worker nodes with the private cloud taint. Since my cluster is hosted in AWS, I'll use the AWS console to stop the instance. Again, the scheduler sees the current state doesn't match the desired state so it starts another pod on one of the available worker nodes. What you may not notice in this video is that it takes over 5 minutes before the new pods are created. (The video is playing 10x speed.) This is due to the default <code>pod-eviction-timeout</code> of 5m0s along with a few other related parameters. For a detailed discussion of this topic and what you can do to reduce it, say from 5m40s to 46s, see <a href="https://fatalfailure.wordpress.com/2016/06/10/improving-kubernetes-reliability-quicker-detection-of-a-node-down/">Improving Kubernetes reliability: quicker detection of a Node down</a>.</p>
<p>Finally, to simulate a site or datacenter failure, I'll again use the AWS console to stop the other worker node with the private cloud taint. As before, another pod is started. More significantly, the last two scenarios can be viewed as disaster recovery. One site, our private cloud, is down and all the work has been migrated to our other site, our public cloud. This will work in any similar situation, i.e. two on-premises datacenters, an on-premises datacenter with a co-location facility, an on-premises datacenter with a public cloud (hybrid cloud), or two public clouds (multi-cloud).</p>
<h2>Resetting</h2>
<p>We'll reset everything back to the way we started by:</p>
<ol>
<li>Deleting the service and deployment we created; and</li>
<li>Removing the taints from the worker nodes.</li>
</ol>
<pre><code class="language-bash">$ kubectl delete -f kens-service.yaml
$ kubectl delete -f kens-deployment.yaml
$ kubectl taint nodes ip-172-30-14-227.us-east-2.compute.internal cloud:NoSchedule-
$ kubectl taint nodes ip-172-30-14-227.us-east-2.compute.internal cloud:NoSchedule-
$ kubectl taint nodes ip-172-30-23-45.us-east-2.compute.internal cloud:NoSchedule-
$ kubectl taint nodes ip-172-30-23-45.us-east-2.compute.internal cloud:NoSchedule-
</code></pre>
<h2>Summary</h2>
<p>We've now seen how using Kubernetes as your container orchestrator makes it easier for an operations or DevOps team (or, in many cases today, a CI/CD pipeline) to manage applications in production. There are a lot more options and features available to you. If you want or need help, Capstone IT is a Docker Premier Consulting Partner as well as being an Azure Gold and AWS Select partner. If you are interested in finding out more and getting help with your Container, Cloud and DevOps transformation, please <a href="https://capstonec.com/contact-us/">Contact Us</a>.</p>
<p>Ken Rider
Solutions Architect
Capstone IT</p>
